{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Notebook\n",
    "This notebook looks to execute on the analyses listed here: https://microsoftapc-my.sharepoint.com/personal/mtremeer_microsoft_com/_layouts/15/doc.aspx?sourcedoc={00cb3c27-ee54-43f4-87ba-1017f94635da}&action=edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import enum\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from helpers import MODEL, DEPLOYMENT_TYPE\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "combined_logs_save_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Combine Log files\n",
    "If logs have not yet been processed from separate JSON files into a single CSV, run this code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hashlib\n",
    "# from pathlib import Path\n",
    "\n",
    "# log_dir = Path(\"logs\")\n",
    "\n",
    "# # Create hash of all files in directory so we can geenrate a unique combined_logs files\n",
    "# all_filenames = \",\".join(str(path) for path in log_dir.rglob(\"*.log\"))\n",
    "# dir_hash = hashlib.md5(all_filenames.encode()).hexdigest()[:8]\n",
    "# combined_logs_save_path = f\"../../logs/all_runs_combined_{dir_hash}.csv\"\n",
    "\n",
    "# # Combine all logs\n",
    "# !cd .. && python -m combine_logs ../../logs $combined_logs_save_path --load-recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTU_COST_PER_UNIT = 123 # Enter the PTU cost per unit here\n",
    "\n",
    "# Create a mapper to convert model info to their key info. Make sure to check that the paygo cost per 1,000 prompt and cost per 1,000 generation tokens fields are correct (paygo_cp1kpt and paygo_cp1kgt)\n",
    "MODEL_CORE_INFO = {\n",
    "    MODEL.GPT_35_TURBO_0613_4K: {\"human_name\": \"GPT-3.5 Turbo 4K 0613\", \"paygo_cp1kpt\": 0.0015, \"paygo_cp1kgt\": 0.002,},\n",
    "    MODEL.GPT_35_TURBO_0613_16K: {\"human_name\": \"GPT-3.5 Turbo 16K 0613\", \"paygo_cp1kpt\": 0.003, \"paygo_cp1kgt\": 0.004,},\n",
    "    MODEL.GPT_35_TURBO_1106_16K: {\"human_name\": \"GPT-3.5 Turbo 1106\", \"paygo_cp1kpt\": 0.001, \"paygo_cp1kgt\": 0.0002,},\n",
    "    MODEL.GPT_4_0613_8K: {\"human_name\": \"GPT-4 8K 0613\", \"paygo_cp1kpt\": 0.03, \"paygo_cp1kgt\": 0.06,},\n",
    "    MODEL.GPT_4_0613_32K: {\"human_name\": \"GPT-4 32K 0613\", \"paygo_cp1kpt\": 0.06, \"paygo_cp1kgt\": 0.12,},\n",
    "    MODEL.GPT_4_TURBO_1106_128K: {\"human_name\": \"GPT-4 Turbo 1106\", \"paygo_cp1kpt\": 0.01, \"paygo_cp1kgt\": 0.03,},\n",
    "    MODEL.GPT_4_TURBO_VISION_1106: {\"human_name\": \"GPT-4 Turbo Vision 1106\", \"paygo_cp1kpt\": 0.01, \"paygo_cp1kgt\": 0.03,},\n",
    "}\n",
    "if len(MODEL_CORE_INFO) != len(MODEL):\n",
    "    print(\"Warning: MODEL_CORE_INFO is missing models. See helpers.py for a list of models.\")\n",
    "\n",
    "# Map resource name to region. This helps us know the local datetime during the test\n",
    "RESOURCE_INFO = {\n",
    "    \"aoai-aueast\": {\"region\": \"australiaeast\", \"deployment_type\": DEPLOYMENT_TYPE.PAYGO},\n",
    "    \"aoai-sweden-mt\": {\"region\": \"swedencentral\", \"deployment_type\": DEPLOYMENT_TYPE.PAYGO},\n",
    "    \"gbb-ea-openai-swedencentral-02\": {\"region\": \"swedencentral\", \"deployment_type\": DEPLOYMENT_TYPE.PTU},\n",
    "}\n",
    "\n",
    "# Map deployment name to key variables that we'll need later\n",
    "DEPLOYMENT_INFO = {\n",
    "    ### Example deployment\n",
    "    # \"gpt-4-ptu\": { # Name of deployment\n",
    "    #     \"model\": MODEL.GPT_4_0613_8K, # Model name - see MODEL definition in helpers.py for more options\n",
    "    #     \"configured_paygo_tpm\": None, # If deployment is PayGO, enter the configured Max TPM here\n",
    "    #     \"configured_ptu_units\": 100, # If deployment is PTU, enter the Number of PTU units here\n",
    "    #     \"paygo_dynamic_quota\": False, # Whether PayGO Dynamic Quota was enabled \n",
    "    #     \"content_filtering_enabled\": False, # Whether PayGO Dynamic Quota was enabled \n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if combined_logs_save_path:\n",
    "    # Use this line if logs have just been combined in the previous cell\n",
    "    combined_logs_path = combined_logs_save_path\n",
    "else:\n",
    "    # Use this line of loading manually\n",
    "    combined_logs_path = Path(\"~/Work Downloads/Customedata_AIA/aia_testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ouput directory for all data and plots generated in this notebook\n",
    "REPO_BASE_DIR = Path.cwd().parent.parent.parent\n",
    "DEFAULT_ANALYSIS_SAVE_DIR = REPO_BASE_DIR / \"analysis_outputs\"\n",
    "combined_logs_name = combined_logs_path.name.split(\".\")[0]\n",
    "CURRENT_ANALYSIS_SAVE_DIR = DEFAULT_ANALYSIS_SAVE_DIR / combined_logs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 logs loaded\n",
      "14 logs remaining after removing early-terminations and those missing data. \n"
     ]
    }
   ],
   "source": [
    "# Load all logs\n",
    "all_logs = pd.read_csv(combined_logs_path)\n",
    "print(f\"{len(all_logs)} logs loaded\")\n",
    "\n",
    "# Remove entries that were early-terminated\n",
    "all_logs = all_logs[~all_logs[\"early_terminated\"]]\n",
    "\n",
    "# Remove entries that have no TPM data (aggregation window might have been too small)\n",
    "all_logs = all_logs[~all_logs[\"tpm_context\"].isna()]\n",
    "print(f\"{len(all_logs)} logs remaining after removing early-terminations and those missing data. \")\n",
    "\n",
    "# Drop unneeded cols\n",
    "all_logs.drop(columns=[\"early_terminated\", \"api_version\", \"frequency_penalty\", \"presence_penalty\", \"temperature\", \"top_p\", \"output_format\"], inplace=True)\n",
    "\n",
    "# Extract region from endpoint_name and add local time\n",
    "all_logs[\"aoai_resource\"] = all_logs[\"api_base_endpoint\"].apply(lambda x: x.split('.')[0].split(\"//\")[1])\n",
    "\n",
    "# Create token profile / replay / combined strings for easier groupby\n",
    "all_logs[\"token_profile\"] = all_logs.apply(lambda row: f\"{int(row['context_tokens'])} / {int(row['max_tokens'])}\" if row[\"context_generation_method\"] == \"generate\" else f\"{int(row['context_tpr_avg'])} / {int(row['max_tokens'])}\", axis=1)\n",
    "all_logs[\"replay_name\"] = all_logs.apply(lambda row: os.path.basename(row['replay_path']).split(\".\")[0] if row[\"context_generation_method\"] == \"replay\" else \"\", axis=1)\n",
    "all_logs[\"workload_name_config\"] = all_logs.apply(lambda row: row[\"token_profile\"] if row[\"context_generation_method\"] == \"generate\" else f'{row[\"replay_name\"]} ({row[\"context_tpr_avg\"]} / {row[\"max_tokens\"]})', axis=1)\n",
    "all_logs[\"workload_name_config_with_break\"] = all_logs.apply(lambda row: row[\"token_profile\"] if row[\"context_generation_method\"] == \"generate\" else f'{row[\"replay_name\"]}<br>({row[\"context_tpr_avg\"]} / {row[\"max_tokens\"]})', axis=1)\n",
    "all_logs[\"workload_name_observed\"] = all_logs.apply(lambda row: row[\"token_profile\"] if row[\"context_generation_method\"] == \"generate\" else f'{row[\"replay_name\"]} ({row[\"context_tpr_avg\"]} / {row[\"gen_tpr_avg\"]})', axis=1)\n",
    "all_logs[\"workload_name_observed_with_break\"] = all_logs.apply(lambda row: row[\"token_profile\"] if row[\"context_generation_method\"] == \"generate\" else f'{row[\"replay_name\"]}<br>({row[\"context_tpr_avg\"]} / {row[\"gen_tpr_avg\"]})', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_ptu  run_date  \n",
       "True    2024-01-19    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add date of run to DF\n",
    "all_logs[\"run_date\"] = all_logs[\"filename\"].apply(lambda x: x.split(\"/\")[-1][:10])\n",
    "all_logs[\"is_ptu\"] = all_logs[\"deployment\"].apply(lambda x: \"ptu\" in x)\n",
    "all_logs.groupby([\"is_ptu\", \"run_date\"]).apply(lambda df: len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logs[[\"context_generation_method\", \"deployment\", \"run_date\", \"workload_name_config\", \"duration\"]].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that all required data has been loaded into the Mappers\n",
    "\n",
    "We'll use these mappers to make sure all models have associated information, and make it easy to enrich our DF/make readable graphs laer on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! All Azure AOAI resources in logs have a RESOURCE_INFO entry.\n",
      "\n",
      "Warning: Model gpt-4-ptu has no entry in DEPLOYMENT_INFO\n",
      "Great! All deployments logs have a DEPLOYMENT_INFO entry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resources_missing = False\n",
    "for resource in all_logs[\"aoai_resource\"].unique():\n",
    "    if resource not in RESOURCE_INFO:\n",
    "        print(f\"WARNING: Azure AOAI resource {resource} has no entry in RESOURCE_INFO\")\n",
    "        resources_missing = True\n",
    "if not resources_missing:\n",
    "    print(\"Great! All Azure AOAI resources in logs have a RESOURCE_INFO entry.\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Check that all models and resources in logs have a mapping entry\n",
    "models_missing = False\n",
    "for deployment in all_logs[\"deployment\"].unique():\n",
    "    if deployment not in DEPLOYMENT_INFO:\n",
    "        print(f\"Warning: Model {deployment} has no entry in DEPLOYMENT_INFO\")\n",
    "        model_missing = True\n",
    "if not models_missing:\n",
    "    print(\"Great! All deployments logs have a DEPLOYMENT_INFO entry.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested Max PTU RPM for each workload\n",
    "max_ptu_rpm_by_workload_mapper = {\n",
    "    (3500, 300): 10,\n",
    "    (500, 100): 49,\n",
    "    (3800, 5): 16,\n",
    "    (3000, 5): 20,\n",
    "    (1000, 5): 62,\n",
    "    (1000, 100): 35,\n",
    "    (1000, 250): 21,\n",
    "    (1000, 500): 12,\n",
    "    (1000, 1000): 4,\n",
    "    (500, 1000): 5,\n",
    "    (250, 1000): 5,\n",
    "    (100, 1000): 5,\n",
    "    (5950, 1000): 4,\n",
    "    (5950, 150): 8,\n",
    "    (6311, 1000): 4,\n",
    "    (6311, 150): 7,\n",
    "}\n",
    "\n",
    "# TODO: Turn this into a mapper and apply it to the DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data - add common fields/transformations\n",
    "\n",
    "* Merge info from mappers\n",
    "* Add fields for observed values from model vs what was requested for the benchmark\n",
    "* Calculate required fields for TCO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpers\n",
    "\n",
    "def format_val_to_k(val):\n",
    "    return f\"{int(val/1000)}K\"\n",
    "\n",
    "\n",
    "def int_to_thousands(val):\n",
    "    if np.inf == val:\n",
    "        return \"inf\"\n",
    "    if np.isnan(val):\n",
    "        return \"NaN\"\n",
    "    if val < 1000:\n",
    "        return f\"{int(val)}\"\n",
    "    if val < 10000:\n",
    "        return f\"{round(val/1000, 1)}K\"\n",
    "    if val >= 1000000:\n",
    "        return f\"{round(val/1000000, 2)}M\"\n",
    "    return f\"{int(val/1000)}K\"\n",
    "\n",
    "def pretty_str(s):\n",
    "    \"\"\"Convert a string to title case and replace underscores with spaces\"\"\"\n",
    "    new = s.replace(\"_\", \" \").title()\n",
    "    capitalize_words = [\"Rpm\", \"Tpm\", \"Tpr\", \"Ptu\", \"Ttft\", \"Tbt\", \"Gpt\"]\n",
    "    for word in capitalize_words:\n",
    "        new = new.replace(word, word.upper())\n",
    "    new = new.replace(\"Paygo\", \"PayGO\")\n",
    "    return new\n",
    "\n",
    "deployment_type_str_mapper = {\n",
    "    DEPLOYMENT_TYPE.PAYGO: \"PayGO\",\n",
    "    DEPLOYMENT_TYPE.PTU: \"PTU\",\n",
    "}\n",
    "\n",
    "def  round_floats(val):\n",
    "    if np.inf == val:\n",
    "        return \"inf\"\n",
    "    if np.isnan(val):\n",
    "        return \"NaN\"\n",
    "    if not (isinstance(val, int) or isinstance(val, float)):\n",
    "        return val\n",
    "    if val < 1:\n",
    "        return f\"{round(val, 3)}\"\n",
    "    if val < 10:\n",
    "        return f\"{round(val, 2)}\"\n",
    "    if val < 100:\n",
    "        return f\"{round(val, 1)}\"\n",
    "    if val < 1000:\n",
    "        return f\"{round(val)}\"\n",
    "    return int_to_thousands(val)\n",
    "\n",
    "def apply_pretty_formatting(df: pd.DataFrame, cols_to_ignore = None) -> pd.DataFrame:\n",
    "    \"\"\"Apply pretty string formatting before displaying a DF\"\"\"\n",
    "    if not cols_to_ignore:\n",
    "        cols_to_ignore = list()\n",
    "    out = df.copy()\n",
    "    numeric_cols = out.select_dtypes(include=np.number).columns\n",
    "    out.loc[:, numeric_cols] = out.loc[:, numeric_cols].apply(lambda x: pd.to_numeric(x, downcast='integer'))\n",
    "    # Format values\n",
    "    for col in [c for c in out.columns if c not in cols_to_ignore]:\n",
    "        try:\n",
    "            if col == \"deployment_type\":\n",
    "                out[col] = out[col].apply(lambda x: deployment_type_str_mapper[x])\n",
    "            if col in [\"total_tpm_relative_to_expected\", \"ptu_rpm_relative_to_expected\", \"observed_prompt_vs_gen_token_ratio\", \"Observed Context/Generation Token Ratio\"]:\n",
    "                out[col] = out[col].apply(lambda x: \"{:,.2f}%\".format(x*100) if x*100 < 100 else (\"{:,.1f}%\".format(x*100) if x*100 < 1000 else \"{:,.0f}%\".format(x*100)))\n",
    "            elif isinstance(col, str) and \"cost\" in col:\n",
    "                out[col] = out[col].apply(lambda x: \"${:,.3f}\".format(x))\n",
    "            elif (isinstance(col, tuple) and \"cost\" in col[0]) or (isinstance(col, str) and \"cost\" in col):\n",
    "                out[col] = out[col].apply(lambda x: \"${:,.3f}\".format(x))\n",
    "            else:\n",
    "                out[col] = out[col].apply(round_floats)\n",
    "        except Exception as e:\n",
    "            print(f\"Transform of col '{col}' failed: {e}\")\n",
    "                \n",
    "    # Format column names\n",
    "    new_cols = []\n",
    "    for col in out.columns.tolist():\n",
    "        if isinstance(col, str):\n",
    "            new_cols.append(pretty_str(col))\n",
    "        else:\n",
    "            new_col = []\n",
    "            for sub_col in col:\n",
    "                new_col.append(pretty_str(sub_col))\n",
    "            new_cols.append(tuple(new_col))\n",
    "    if out.columns.nlevels == 1:\n",
    "        out.columns = new_cols\n",
    "    else:\n",
    "        names = out.columns.names\n",
    "        out.columns = pd.MultiIndex.from_tuples(new_cols, names = names)\n",
    "    # out.columns = pd.MultiIndex.from_tuples(out.columns, names = names)\n",
    "        \n",
    "    # Index names\n",
    "    if out.index.nlevels == 1:\n",
    "        # out.index = new_idxs\n",
    "        out.index = pd.Index(out.index, name = pretty_str(out.index.name) if out.index.name else out.index.name)\n",
    "    else:\n",
    "        names = [pretty_str(name) for name in out.index.names]\n",
    "        out.index = pd.MultiIndex.from_tuples(out.index.tolist(), names = names)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess all data\n",
    "# Merge info from RESOURCE_INFO\n",
    "def merge_resource_info(row):\n",
    "    resource = row[\"aoai_resource\"]\n",
    "    if resource in RESOURCE_INFO:\n",
    "        resource_info = RESOURCE_INFO[resource]\n",
    "        for key, value in resource_info.items():\n",
    "            row[key] = value\n",
    "    return row\n",
    "all_logs = all_logs.apply(merge_resource_info, axis=1)\n",
    "\n",
    "# Merge info from DEPLOYMENT_INFO\n",
    "def merge_deployment_info(row):\n",
    "    deployment = row[\"deployment\"]\n",
    "    if deployment in DEPLOYMENT_INFO:\n",
    "        model_info = DEPLOYMENT_INFO[deployment]\n",
    "        for key, value in model_info.items():\n",
    "            row[key] = value\n",
    "    return row\n",
    "all_logs = all_logs.apply(merge_deployment_info, axis=1)\n",
    "\n",
    "# Merge info from MODEL_CORE_INFO\n",
    "def merge_model_core_info(row):\n",
    "    model = row[\"model\"]\n",
    "    if model in MODEL_CORE_INFO:\n",
    "        model_core_info = MODEL_CORE_INFO[model]\n",
    "        for key, value in model_core_info.items():\n",
    "            row[key] = value\n",
    "    return row\n",
    "all_logs = all_logs.apply(merge_model_core_info, axis=1)\n",
    "\n",
    "# Add ratio of prompt vs generated tokens for each run (using actual generated token values)\n",
    "all_logs[\"observed_prompt_vs_gen_token_ratio\"] = all_logs[\"context_tpr_avg\"] / (all_logs[\"context_tpr_avg\"] + all_logs[\"gen_tpr_avg\"])\n",
    "\n",
    "# Calculate time between prompt tokens\n",
    "all_logs[\"tbt_prompt_avg\"] = all_logs[\"ttft_avg\"] / all_logs[\"context_tokens\"]\n",
    "all_logs[\"tbt_prompt_95th\"] = all_logs[\"ttft_95th\"] / all_logs[\"context_tokens\"]\n",
    "\n",
    "# Forecast the expected e2e latency if the model predicted the full number of max_tokens (multiple ttft * number of missing tokens and add on to the existing e2e time)\n",
    "all_logs[\"e2e_avg_assuming_max_tokens\"] = all_logs.apply(lambda row: row[\"e2e_avg\"] + ((row[\"max_tokens\"] - row[\"gen_tpr_avg\"]) * row[\"tbt_avg\"]), axis=1)\n",
    "\n",
    "# Total time from first token to last token\n",
    "all_logs[\"tfft_to_lt_avg\"] = all_logs[\"e2e_avg\"] - all_logs[\"ttft_avg\"]\n",
    "all_logs[\"tfft_to_lt_95th\"] = all_logs[\"e2e_95th\"] - all_logs[\"ttft_95th\"]\n",
    "all_logs[\"tfft_to_lt_avg_assuming_max_tokens\"] = all_logs[\"e2e_avg_assuming_max_tokens\"] - all_logs[\"ttft_avg\"]\n",
    "\n",
    "# Estimate Average concurrency\n",
    "# For runs with a low number of RPM, the PTU may not be fully utilised. We will attempt to calculate an effective utilisation based on the number of throttled requests.\n",
    "all_logs[\"minutely_gpu_time\"] = all_logs.apply(lambda row: row[\"e2e_avg\"] * row[\"rpm\"], axis=1)\n",
    "all_logs[\"avg_concurrency\"] = all_logs[\"minutely_gpu_time\"].apply(lambda x: round(x / 60, 1))\n",
    "\n",
    "# TPM while in context or generation (to prevent TPM for prompt being reduced while waiting for generation)\n",
    "all_logs[\"tpm_context_exclusive\"] = all_logs[\"context_tpr_avg\"] / all_logs[\"ttft_avg\"] * 60 * all_logs[\"avg_concurrency\"]\n",
    "all_logs[\"tpm_gen_exclusive_per_request\"] = all_logs[\"gen_tpr_avg\"] / all_logs[\"tfft_to_lt_avg\"] * 60\n",
    "all_logs[\"tpm_gen_exclusive\"] = all_logs[\"tpm_gen_exclusive_per_request\"] * all_logs[\"avg_concurrency\"]\n",
    "\n",
    "# Add throttled RPM\n",
    "all_logs[\"successful\"] = all_logs[\"completed\"] - all_logs[\"failures\"]\n",
    "all_logs[\"throttled_rpm\"] = all_logs[\"throttled\"] / all_logs[\"aggregation_window\"] * 60\n",
    "\n",
    "# Other helper cols\n",
    "all_logs[\"human_name_w_tpm\"] = all_logs.apply(\n",
    "    lambda row: f\"{row['human_name']} - ({int(row['configured_ptu_units'])} PTUs)\" if (row['deployment_type'] == DEPLOYMENT_TYPE.PTU) else f\"{row['human_name']} - PayGO ({format_val_to_k(row['configured_paygo_tpm'])} Max TPM)\", axis=1\n",
    ")\n",
    "all_logs[\"human_name_w_deployment_type\"]  = all_logs.apply(\n",
    "    lambda row: f\"{row['human_name']} - PTU\" if (row['deployment_type'] == DEPLOYMENT_TYPE.PTU) else f\"{row['human_name']} - PayGO \", axis=1\n",
    ")\n",
    "\n",
    "### Estimate cost over time\n",
    "# Estimate hourly and monthly cost based on deployment type\n",
    "MINUTES_PER_MONTH = 60 * 24 * 365.25 / 12\n",
    "all_logs[\"paygo_cost_per_request\"] = all_logs.apply(lambda row: round((row[\"context_tpr_avg\"] * row[\"paygo_cp1kpt\"] / 1000) + (row[\"gen_tpr_avg\"] * row[\"paygo_cp1kgt\"] / 1000), 4), axis=1)\n",
    "all_logs[\"paygo_cost_per_month\"] = all_logs[\"paygo_cost_per_request\"] * all_logs[\"rpm\"] * MINUTES_PER_MONTH\n",
    "all_logs[\"paygo_cost_per_hour\"] = all_logs[\"paygo_cost_per_request\"] * all_logs[\"rpm\"] * 60\n",
    "all_logs[\"est_monthly_cost\"] = all_logs.apply(lambda row: round(row[\"configured_ptu_units\"] * PTU_COST_PER_UNIT) if row[\"deployment_type\"] is DEPLOYMENT_TYPE.PTU else row[\"paygo_cost_per_month\"], axis=1)\n",
    "all_logs[\"est_hourly_cost\"] = all_logs.apply(lambda row: round(row[\"est_monthly_cost\"] / MINUTES_PER_MONTH * 60, 3), axis=1)\n",
    "\n",
    "# Estimate cost per request\n",
    "all_logs[\"est_cost_per_request\"] = all_logs.apply(lambda row: round(row[\"est_hourly_cost\"] / 60 / row[\"rpm\"], 4) if row[\"deployment_type\"] is DEPLOYMENT_TYPE.PTU else row[\"paygo_cost_per_request\"], axis=1)\n",
    "all_logs[\"est_cost_per_1k_requests\"] = all_logs[\"est_cost_per_request\"] * 1000\n",
    "all_logs[\"paygo_cost_per_1k_requests\"] = all_logs[\"paygo_cost_per_request\"] * 1000\n",
    "# Compare PTU and PayGO cost\n",
    "all_logs[\"ptu_cost_vs_paygo_cost\"] = all_logs[\"est_cost_per_request\"] / all_logs[\"paygo_cost_per_request\"]\n",
    "\n",
    "# Add expected RPM/TPM and comparison\n",
    "all_logs[\"exp_ptu_rpm\"] = all_logs.apply(lambda row: max_ptu_rpm_by_workload_mapper.get((row[\"context_tpr_avg\"], row[\"max_tokens\"])), axis=1)\n",
    "all_logs[\"exp_ptu_tpm\"] = all_logs.apply(lambda row: row[\"exp_ptu_rpm\"] * (row[\"context_tpr_avg\"] + row[\"max_tokens\"]) if row[\"deployment_type\"] is DEPLOYMENT_TYPE.PTU else None, axis=1)\n",
    "all_logs[\"ptu_rpm_relative_to_expected\"] = all_logs[\"rpm\"] / all_logs[\"exp_ptu_rpm\"]\n",
    "all_logs[\"exp_max_tpm\"] = all_logs.apply(lambda row: row[\"exp_ptu_tpm\"] if row[\"deployment_type\"] is DEPLOYMENT_TYPE.PTU else row[\"configured_paygo_tpm\"], axis=1)\n",
    "all_logs[\"total_tpm_relative_to_expected\"] = all_logs[\"tpm_total\"] / all_logs[\"exp_max_tpm\"]\n",
    "\n",
    "# Add float representation of utilisation for PTU\n",
    "all_logs[\"util_avg_float\"] = all_logs[\"util_avg\"].apply(lambda s: float(s.replace(\"%\", \"\")) if isinstance(s, str) else s)\n",
    "all_logs[\"util_95th_float\"] = all_logs[\"util_95th\"].apply(lambda s: float(s.replace(\"%\", \"\")) if isinstance(s, str) else s)\n",
    "\n",
    "# Add col to switch between PTU groups but still include PayGO runs, based on date of run\n",
    "all_logs[\"run_group\"] = all_logs.apply(lambda row: \" or \".join(all_logs[\"run_date\"].unique().tolist()) if row[\"deployment_type\"] is DEPLOYMENT_TYPE.PAYGO else row['run_date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats for the best run: By model and workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics and group-by to get these:\n",
    "\n",
    "Across all runs\n",
    "- e2e avg & p95\n",
    "- TTFT avg & p95\n",
    "- Latency avg & p95\n",
    "- TPM\n",
    "- RPM\n",
    "- Max concurrency\n",
    "- Cost per request\n",
    "\n",
    "Across runs where gen tokens >= 100:\n",
    "- Min/Max Gen TPM/request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_RENAME_MAPPER = {\n",
    "    \"context_tpr_avg\": \"Average Context Tokens/Request\",\n",
    "    \"gen_tpr_avg\": \"Average Generation Tokens/Request\",\n",
    "    \"observed_prompt_vs_gen_token_ratio\": \"Observed Context/Generation Token Ratio\",\n",
    "    \"tpm_total\": \"TPM - Total\",\n",
    "    \"tpm_context_exclusive\": \"TPM - Context Exclusive\",\n",
    "    \"tpm_gen_exclusive\": \"TPM - Generation Exclusive\",\n",
    "    \"tpm_gen_exclusive_per_request\": \"TPM - Generation Exclusive Per Request\",\n",
    "    \"est_cost_per_1k_requests\": \"Cost Per 1K Requests\",\n",
    "    \"tpm_context\": \"TPM - Context\",\n",
    "    \"tpm_gen\": \"TPM - Generation\",\n",
    "    \"ttft_avg\": \"Time-to-First-Token - Average\",\n",
    "    \"ttft_95th\": \"Time-to-First-Token - P95\",\n",
    "    \"e2e_avg\": \"End-to-End Latency - Average\",\n",
    "    \"e2e_95th\": \"End-to-End Latency - P95\",\n",
    "    \"avg_concurrency\": \"Average Concurrent Requests\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"human_name_w_tpm\", \"workload_name_config\"]\n",
    "cols_for_agg = [\"deployment_type\", \"rate\", \"clients\", \"context_tpr_avg\", \"gen_tpr_avg\", \"observed_prompt_vs_gen_token_ratio\", \"tpm_total\", \"tpm_context\", \"tpm_context_exclusive\", \"tpm_gen\", \"tpm_gen_exclusive\", \"tpm_gen_exclusive_per_request\", \"ttft_avg\", \"ttft_95th\", \"e2e_avg\", \"e2e_95th\", \"rpm\", \"est_cost_per_1k_requests\", \"paygo_cost_per_1k_requests\", \"ptu_cost_vs_paygo_cost\"]\n",
    "min_100t_cols_for_agg = [\"tpm_gen_exclusive\", \"tpm_gen_exclusive_per_request\"]\n",
    "\n",
    "# Filter to best runs for each deployment and workload\n",
    "min_max_agg_mask = (all_logs[\"deployment_type\"] == DEPLOYMENT_TYPE.PAYGO) | ((all_logs[\"deployment_type\"] == DEPLOYMENT_TYPE.PTU) & (all_logs[\"throttled\"] == 0))\n",
    "best_runs_df = all_logs[min_max_agg_mask].sort_values([\"tpm_total\"], ascending=False).groupby(group_cols).first()\n",
    "runs_with_low_completion_tokens_mask = best_runs_df[\"gen_tpr_avg\"] < 100\n",
    "runs_to_include_df = best_runs_df[cols_for_agg].rename(columns=COL_RENAME_MAPPER)\n",
    "\n",
    "out = apply_pretty_formatting(runs_to_include_df, cols_to_ignore=[\"context_tpr_avg\", \"gen_tpr_avg\", \"ptu_cost_vs_paygo_cost\", \"Cost Per 1K Requests\", \"paygo_cost_per_1k_requests\"])\n",
    "\n",
    "# Ignore the generation speed TPM for runs with less than 100 completion tokens\n",
    "out.loc[runs_with_low_completion_tokens_mask, \"TPM - Generation Exclusive\"] = \"N/A\"\n",
    "out.loc[runs_with_low_completion_tokens_mask, \"TPM - Generation Exclusive Per Request\"] = \"N/A\"\n",
    "\n",
    "# Format cost columns for PTU and PayGO\n",
    "ptu_mask = out[\"Deployment Type\"] == \"PTU\"\n",
    "out.loc[ptu_mask, \"Cost Per 1K Requests\"] = out.loc[ptu_mask, \"Cost Per 1K Requests\"].apply(lambda x: f\"${round(x, 1)}\")\n",
    "out.loc[ptu_mask, \"PayGO Cost Per 1K Requests\"] = out.loc[ptu_mask, \"PayGO Cost Per 1K Requests\"].apply(lambda x: f\"${round(x, 1)}\")\n",
    "# Get cost for different usage patterns\n",
    "out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost - Weekdays\"] = out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost\"] * 7 / 5\n",
    "out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost - Business Hours (8AM-6PM)\"] = out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost\"] * 7 * 24 / 5 / 10\n",
    "\n",
    "out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost\"] = out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost\"].apply(lambda x: f\"{round(x, 2)}x\")\n",
    "out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost - Weekdays\"] = out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost - Weekdays\"].apply(lambda x: f\"{round(x, 2)}x\")\n",
    "out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost - Business Hours (8AM-6PM)\"] = out.loc[ptu_mask, \"PTU Cost Vs PayGO Cost - Business Hours (8AM-6PM)\"].apply(lambda x: f\"{round(x, 2)}x\")\n",
    "out.rename(columns={\"PTU Cost Vs PayGO Cost\": \"PTU Cost Vs PayGO Cost - 24/7\"}, inplace=True)\n",
    "\n",
    "outpath = CURRENT_ANALYSIS_SAVE_DIR / \"data\" / \"best_run_stats_by_deployment_and_workload.csv\"\n",
    "outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "out.to_csv(outpath, index=True)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: Latency by workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = best_runs_df.reset_index()\n",
    "\n",
    "workloads_to_plot  = plot_df[\"workload_name_observed\"].unique().tolist()\n",
    "legend_gap = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create plot\n",
    "fig = make_subplots(\n",
    "    rows=len(workloads_to_plot), cols=1, \n",
    "    vertical_spacing=0.1, \n",
    "    subplot_titles=workloads_to_plot,\n",
    ")\n",
    "x_axis_max = plot_df[\"e2e_95th\"].max() * 1.1\n",
    "\n",
    "for i, workload_name in enumerate(workloads_to_plot, start=1):\n",
    "    temp_df = plot_df[plot_df[\"workload_name_observed\"] == workload_name].sort_values([\"deployment\"], ascending=False)\n",
    "    x_max = temp_df[\"e2e_95th\"].max()\n",
    "    dtick = round(x_max * 1.03 / 25 * 4) / 4\n",
    "    # Create data\n",
    "    model_labels = []\n",
    "    stat_labels = []\n",
    "\n",
    "    ttft_avg_data = []\n",
    "    e2e_avg_data = []\n",
    "    e2e_95th_data = []\n",
    "    ttft_95th_data = []\n",
    "\n",
    "    for _i, row in temp_df.iterrows():\n",
    "        # 95th\n",
    "        model_labels.append(row[\"human_name_w_deployment_type\"])\n",
    "        stat_labels.append(\"P95\")\n",
    "        ttft_95th_data.append(row[\"ttft_95th\"])\n",
    "        e2e_95th_data.append(row[\"e2e_95th\"] - row[\"ttft_95th\"])\n",
    "        ttft_95th_data.append(0)\n",
    "        e2e_95th_data.append(0)\n",
    "        # Avg\n",
    "        model_labels.append(row[\"human_name_w_deployment_type\"])\n",
    "        stat_labels.append(\"Avg\")\n",
    "        ttft_avg_data.append(0)\n",
    "        e2e_avg_data.append(0)\n",
    "        ttft_avg_data.append(row[\"ttft_avg\"])\n",
    "        e2e_avg_data.append(row[\"e2e_avg\"] - row[\"ttft_avg\"])\n",
    "    axis_labels = [\n",
    "        model_labels,\n",
    "        stat_labels\n",
    "    ]\n",
    "    # e2e latency\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=ttft_avg_data, \n",
    "            y=axis_labels,\n",
    "            name=\"Avg TTFT\",\n",
    "            legendgroup=i,\n",
    "            legendrank=4,\n",
    "            marker={\"color\": \"green\", \"opacity\": 0.8},\n",
    "            orientation='h',\n",
    "            ),\n",
    "        row = i, \n",
    "        col = 1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=e2e_avg_data, \n",
    "            y=axis_labels,\n",
    "            name=\"Avg Generation\",\n",
    "            legendgroup=i,\n",
    "            legendrank=3,\n",
    "            marker={\"color\": \"blue\", \"opacity\": 0.8},\n",
    "            orientation='h',\n",
    "            ),\n",
    "        row = i, \n",
    "        col = 1\n",
    "    )\n",
    "    # P95 latency\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=ttft_95th_data, \n",
    "            y=axis_labels,\n",
    "            name=\"P95 TTFT\",\n",
    "            legendgroup=i,\n",
    "            legendrank=2,\n",
    "            marker={\"color\": \"darkorange\", \"opacity\": 0.9},\n",
    "            orientation='h',\n",
    "            ),\n",
    "        row = i, \n",
    "        col = 1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=e2e_95th_data, \n",
    "            y=axis_labels,\n",
    "            name=\"P95 Generation\",\n",
    "            legendgroup=i,\n",
    "            legendrank=1,\n",
    "            marker={\"color\": \"red\", \"opacity\": 0.8},\n",
    "            orientation='h',\n",
    "            ),\n",
    "        row = i, \n",
    "        col = 1\n",
    "    )\n",
    "    # fig.update_yaxes(title_text=\"Seconds\", row=y, col=1)\n",
    "    fig.update_xaxes(title_text=\"Seconds\", row=i, col=1, dtick=dtick)\n",
    "\n",
    "title = \"Request Latency Metrics by Model and Workload\"\n",
    "height = 1000\n",
    "fig.update_layout(title_text=title, title_x=0.5, height=height, width=1600, barmode='stack', legend_tracegroupgap=legend_gap)\n",
    "fig.show()\n",
    "\n",
    "# Save to disk\n",
    "outpath = CURRENT_ANALYSIS_SAVE_DIR / \"plots/latency_by_workload.png\"\n",
    "outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "fig.write_image(outpath)\n",
    "\n",
    "print(\"Saved to\", outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots by Workload: Maximum performance stats with common workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythondatascience.plavox.info/wp-content/uploads/2016/06/colorpalette.png\n",
    "\n",
    "BAR_MARKER_SETTINGS = {\n",
    "    \"latency_gen\": {\"color\": \"blueviolet\", \"opacity\": 0.8},\n",
    "    \"latency_prompt\": {\"color\": \"blue\", \"opacity\": 0.8},\n",
    "    \"ttft_prompt\": {\"color\": \"grey\", \"opacity\": 0.8},\n",
    "    \"ttft_gen\": {\"color\": \"red\", \"opacity\": 0.8},\n",
    "    \"tpm_prompt\": {\"color\": \"darkorange\", \"opacity\": 0.8},\n",
    "    \"tpm_gen\": {\"color\": \"maroon\", \"opacity\": 0.8},\n",
    "    \"rpm\": {\"color\": \"green\", \"opacity\": 0.8},\n",
    "    \"throttled_rpm\": {\"color\": \"red\", \"opacity\": 0.8},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_set = plot_df[\"deployment\"].drop_duplicates().values.tolist()\n",
    "\n",
    "for deployment in group_set:\n",
    "    temp_df = plot_df[(plot_df[\"deployment\"] == deployment)].sort_values([\"workload_name_observed\", \"deployment\", \"clients\", \"rate\"])\n",
    "    x_axis_title = \"Workload Name\"\n",
    "    title = f\"{temp_df['human_name_w_tpm'].iloc[0]} - Best Run Stats by Workload\"\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2, row_heights=[400, 400], \n",
    "        subplot_titles=(\"Average End-to-End Request Latency\", \"P95 End-to-End Request Latency\", \"Successful & Throttled Requests Per Minute (RPM)\", \"Total Tokens Per Minute (TPM)\")\n",
    "    )\n",
    "    workload_labels = temp_df[\"workload_name_observed_with_break\"].values.tolist()\n",
    "    latency_y_max = temp_df[\"e2e_95th\"].max() * 1.1\n",
    "    # Average Latencies\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels,\n",
    "            y=temp_df[\"ttft_avg\"].values.tolist(),\n",
    "            name=\"Time to First Token\",\n",
    "            legendgroup=0,\n",
    "            marker=BAR_MARKER_SETTINGS[\"latency_prompt\"],\n",
    "            # orientation='h',\n",
    "            ),\n",
    "        row = 1, \n",
    "        col = 1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels, \n",
    "            y=(temp_df[\"e2e_avg\"] - temp_df[\"ttft_avg\"]).values.tolist(),\n",
    "            name=\"Token Generation\",\n",
    "            legendgroup=0,\n",
    "            marker=BAR_MARKER_SETTINGS[\"latency_gen\"],\n",
    "            # orientation='h',\n",
    "            ),\n",
    "        row = 1, \n",
    "        col = 1\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Seconds\", range=[0, latency_y_max], row=1, col=1)\n",
    "    fig.update_xaxes(title_text=x_axis_title, row=1, col=1)\n",
    "\n",
    "    # P95 Latencies\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels, \n",
    "            y=temp_df[\"ttft_95th\"].values.tolist(),\n",
    "            name=\"Token Generation\",\n",
    "            # legendgroup=1,\n",
    "            showlegend = False,\n",
    "            marker=BAR_MARKER_SETTINGS[\"latency_prompt\"],\n",
    "            # orientation='h',\n",
    "            ),\n",
    "        row = 1, \n",
    "        col = 2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels, \n",
    "            y=(temp_df[\"e2e_95th\"] - temp_df[\"ttft_95th\"]).values.tolist(),\n",
    "            name=\"Token Generation\",\n",
    "            # legendgroup=1,\n",
    "            showlegend = False,\n",
    "            marker=BAR_MARKER_SETTINGS[\"latency_gen\"],\n",
    "            # orientation='h',\n",
    "            ),\n",
    "        row = 1, \n",
    "        col = 2\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Seconds\", range=[0, latency_y_max], row=1, col=2)\n",
    "    fig.update_xaxes(title_text=x_axis_title, row=1, col=2)\n",
    "\n",
    "    # RPM\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels, \n",
    "            y=temp_df[\"rpm\"].values.tolist(),\n",
    "            name=deployment,\n",
    "            legendgroup=False,\n",
    "            showlegend = False,\n",
    "            marker=BAR_MARKER_SETTINGS[\"rpm\"],\n",
    "        ),\n",
    "        row = 2, \n",
    "        col = 1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels, \n",
    "            y=temp_df[\"throttled_rpm\"].values.tolist(),\n",
    "            name=deployment,\n",
    "            legendgroup=False,\n",
    "            showlegend = False,\n",
    "            marker=BAR_MARKER_SETTINGS[\"throttled_rpm\"],\n",
    "        ),\n",
    "        row = 2, \n",
    "        col = 1\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"RPM - Log Scale\", type=\"log\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=x_axis_title, row=2, col=1)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels, \n",
    "            y=temp_df[\"tpm_context\"].values.tolist(),\n",
    "            name=\"Context TPM\",\n",
    "            legendgroup=2,\n",
    "            showlegend = True,\n",
    "            marker=BAR_MARKER_SETTINGS[\"tpm_prompt\"],\n",
    "            ),\n",
    "        row = 2, \n",
    "        col = 2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=workload_labels, \n",
    "            y=temp_df[\"tpm_gen\"].values.tolist(),\n",
    "            name=\"Generation TPM\",\n",
    "            legendgroup=2,\n",
    "            showlegend = True,\n",
    "            marker=BAR_MARKER_SETTINGS[\"tpm_gen\"],\n",
    "            ),\n",
    "        row = 2, \n",
    "        col = 2\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"TPM - Log Scale\", type=\"log\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=x_axis_title, row=2, col=2)\n",
    "\n",
    "    # Save short and tall versions\n",
    "    for height, legend_gap in [(600, 220), (800, 350)]:\n",
    "        fig.update_layout(title_text=title, title_x=0.5, height=height, width=1600, coloraxis=dict(colorscale='Bluered_r'), barmode='stack', legend_tracegroupgap=legend_gap)\n",
    "        if height == 800:\n",
    "            fig.show()\n",
    "\n",
    "        # Save to disk\n",
    "        outpath = CURRENT_ANALYSIS_SAVE_DIR / f\"plots/workload_stats_2x2_{deployment}_{height}px.png\"\n",
    "        outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "        fig.write_image(outpath)\n",
    "\n",
    "        print(\"Saved to\", outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_set = common_workload_df[[\"deployment\", \"run_date\"]].drop_duplicates().values.tolist()\n",
    "\n",
    "# for deployment, run_date in group_set:\n",
    "#     temp_df = common_workload_df[(common_workload_df[\"deployment\"] == deployment)].sort_values([\"workload_name\", \"deployment\", \"clients\", \"rate\"])\n",
    "#     # Determine whether to use clients or rate as the x-axis\n",
    "#     num_clients = temp_df[\"clients\"].nunique()\n",
    "#     num_rates = temp_df[\"rate\"].nunique()\n",
    "#     if num_clients > 1 and num_rates > 1:\n",
    "#         print(f\"Skipping {deployment} as it has multiple clients and rates\")\n",
    "#         continue\n",
    "#     elif num_clients > 1:\n",
    "#         group_col = \"clients\"\n",
    "#         x_axis_title = \"Workload Profile and Number of Clients (No Rate Limit)\"\n",
    "#         title = f\"{temp_df['human_name_w_tpm'].iloc[0]} - Common Workloads with Various Levels of Concurrency\"\n",
    "#     else:\n",
    "#         group_col = \"rate\"\n",
    "#         x_axis_title = \"Workload Profile and Max Request RPM\"\n",
    "#         title = f\"{temp_df['human_name_w_tpm'].iloc[0]} - Common Workloads with Various Request Rates\"\n",
    "#     fig = make_subplots(\n",
    "#         rows=2, cols=2, row_heights=[400, 400], \n",
    "#         subplot_titles=(\"Average End-to-End Request Latency\", \"P95 End-to-End Request Latency\", \"Successful & Throttled Requests Per Minute (RPM)\", \"Total Tokens Per Minute (TPM)\")\n",
    "#     )\n",
    "#     workload_labels = [\n",
    "#         temp_df[\"workload_name\"].values.tolist(),\n",
    "#         temp_df[group_col].values.tolist(),\n",
    "#     ]\n",
    "#     latency_y_max = temp_df[\"e2e_95th\"].max() * 1.1\n",
    "#     # Average Latencies\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels,\n",
    "#             y=temp_df[\"ttft_avg\"].values.tolist(),\n",
    "#             name=\"Time to First Token\",\n",
    "#             legendgroup=0,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"latency_prompt\"],\n",
    "#             # orientation='h',\n",
    "#             ),\n",
    "#         row = 1, \n",
    "#         col = 1\n",
    "#     )\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels, \n",
    "#             y=(temp_df[\"e2e_avg\"] - temp_df[\"ttft_avg\"]).values.tolist(),\n",
    "#             name=\"Token Generation\",\n",
    "#             legendgroup=0,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"latency_gen\"],\n",
    "#             # orientation='h',\n",
    "#             ),\n",
    "#         row = 1, \n",
    "#         col = 1\n",
    "#     )\n",
    "#     fig.update_yaxes(title_text=\"Seconds\", range=[0, latency_y_max], row=1, col=1)\n",
    "#     fig.update_xaxes(title_text=x_axis_title, row=1, col=1)\n",
    "\n",
    "#     # P95 Latencies\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels, \n",
    "#             y=temp_df[\"ttft_95th\"].values.tolist(),\n",
    "#             name=\"Token Generation\",\n",
    "#             # legendgroup=1,\n",
    "#             showlegend = False,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"latency_prompt\"],\n",
    "#             # orientation='h',\n",
    "#             ),\n",
    "#         row = 1, \n",
    "#         col = 2\n",
    "#     )\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels, \n",
    "#             y=(temp_df[\"e2e_95th\"] - temp_df[\"ttft_95th\"]).values.tolist(),\n",
    "#             name=\"Token Generation\",\n",
    "#             # legendgroup=1,\n",
    "#             showlegend = False,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"latency_gen\"],\n",
    "#             # orientation='h',\n",
    "#             ),\n",
    "#         row = 1, \n",
    "#         col = 2\n",
    "#     )\n",
    "#     fig.update_yaxes(title_text=\"Seconds\", range=[0, latency_y_max], row=1, col=2)\n",
    "#     fig.update_xaxes(title_text=x_axis_title, row=1, col=2)\n",
    "\n",
    "#     # RPM\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels, \n",
    "#             y=temp_df[\"rpm\"].values.tolist(),\n",
    "#             name=deployment,\n",
    "#             legendgroup=False,\n",
    "#             showlegend = False,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"rpm\"],\n",
    "#         ),\n",
    "#         row = 2, \n",
    "#         col = 1\n",
    "#     )\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels, \n",
    "#             y=temp_df[\"throttled_rpm\"].values.tolist(),\n",
    "#             name=deployment,\n",
    "#             legendgroup=False,\n",
    "#             showlegend = False,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"throttled_rpm\"],\n",
    "#         ),\n",
    "#         row = 2, \n",
    "#         col = 1\n",
    "#     )\n",
    "#     fig.update_yaxes(title_text=\"RPM - Log Scale\", type=\"log\", row=2, col=1)\n",
    "#     fig.update_xaxes(title_text=x_axis_title, row=2, col=1)\n",
    "\n",
    "#     # TPM\n",
    "#     # if temp_df[\"deployment_type\"].iloc[0] is DEPLOYMENT_TYPE.PAYGO:\n",
    "#     #     # Add dotted line for congigured PayGO TPM. Use both a plot and a shape to get a legend and a full height vline\n",
    "#     #     configured_tpm = temp_df[\"configured_paygo_tpm\"].iloc[0]\n",
    "#     #     fig.add_trace(\n",
    "#     #         go.Scatter(\n",
    "#     #             y=[configured_tpm] * len(workload_labels),\n",
    "#     #             x=workload_labels,\n",
    "#     #             name=f\"Configured PayGO TPM ({int_to_thousands(int(configured_tpm))})\",\n",
    "#     #             legendgroup=2,\n",
    "#     #             line=dict(color='green', width=2, dash='5px'),\n",
    "#     #             marker=dict(color='rgba(255, 0, 0, 0)')\n",
    "#     #             ),\n",
    "#     #         row = 2, \n",
    "#     #         col = 2\n",
    "#     #     )\n",
    "#     #     fig.add_hline(\n",
    "#     #         y=configured_tpm,\n",
    "#     #         line_dash=\"dot\",\n",
    "#     #         showlegend=False,\n",
    "#     #         row = 2, \n",
    "#     #         col = 2,\n",
    "#     #         line=dict(color='green', width=2, dash='5px')\n",
    "#     #     )\n",
    "#     # else:\n",
    "#     #     # Add dotted line for congigured PTU TPM (min and max). Use both a plot and a shape to get a legend and a full height vline\n",
    "#     #     exp_min_ptu_tpm = temp_df[\"exp_min_ptu_tpm\"].iloc[0]\n",
    "#     #     exp_max_ptu_tpm = temp_df[\"exp_max_ptu_tpm\"].iloc[0]\n",
    "#     #     fig.add_trace(\n",
    "#     #         go.Scatter(\n",
    "#     #             y=[exp_max_ptu_tpm] * len(workload_labels),\n",
    "#     #             x=workload_labels,\n",
    "#     #             name=f\"Min Expected PTU TPM ({int_to_thousands(int(exp_min_ptu_tpm))})\",\n",
    "#     #             legendgroup=2,\n",
    "#     #             line=dict(color='green', width=2, dash='5px'),\n",
    "#     #             marker=dict(color='rgba(255, 0, 0, 0)')\n",
    "#     #             ),\n",
    "#     #         row = 2, \n",
    "#     #         col = 2\n",
    "#     #     )\n",
    "#     #     fig.add_hline(\n",
    "#     #         y=exp_min_ptu_tpm,\n",
    "#     #         line_dash=\"dot\",\n",
    "#     #         showlegend=False,\n",
    "#     #         row = 2, \n",
    "#     #         col = 2,\n",
    "#     #         line=dict(color='green', width=2, dash='5px'),\n",
    "#     #     )\n",
    "#     #     fig.add_trace(\n",
    "#     #         go.Scatter(\n",
    "#     #             y=[exp_max_ptu_tpm] * len(workload_labels),\n",
    "#     #             x=workload_labels,\n",
    "#     #             name=f\"Max Expected PTU TPM ({int_to_thousands(int(exp_max_ptu_tpm))})\",\n",
    "#     #             legendgroup=2,\n",
    "#     #             line=dict(color='purple', width=2, dash='5px'),\n",
    "#     #             marker=dict(color='rgba(255, 0, 0, 0)')\n",
    "#     #             ),\n",
    "#     #         row = 2, \n",
    "#     #         col = 2\n",
    "#     #     )\n",
    "#     #     fig.add_hline(\n",
    "#     #         y=exp_max_ptu_tpm,\n",
    "#     #         line_dash=\"dot\",\n",
    "#     #         showlegend=False,\n",
    "#     #         row = 2, \n",
    "#     #         col = 2,\n",
    "#     #         line=dict(color='purple', width=2, dash='5px'),\n",
    "#     #     )\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels, \n",
    "#             y=temp_df[\"tpm_context\"].values.tolist(),\n",
    "#             name=\"Context TPM\",\n",
    "#             legendgroup=2,\n",
    "#             showlegend = True,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"tpm_prompt\"],\n",
    "#             ),\n",
    "#         row = 2, \n",
    "#         col = 2\n",
    "#     )\n",
    "#     fig.add_trace(\n",
    "#         go.Bar(\n",
    "#             x=workload_labels, \n",
    "#             y=temp_df[\"tpm_gen\"].values.tolist(),\n",
    "#             name=\"Generation TPM\",\n",
    "#             legendgroup=2,\n",
    "#             showlegend = True,\n",
    "#             marker=BAR_MARKER_SETTINGS[\"tpm_gen\"],\n",
    "#             ),\n",
    "#         row = 2, \n",
    "#         col = 2\n",
    "#     )\n",
    "#     fig.update_yaxes(title_text=\"TPM - Log Scale\", type=\"log\", row=2, col=2)\n",
    "#     fig.update_xaxes(title_text=x_axis_title, row=2, col=2)\n",
    "\n",
    "\n",
    "#     # Save short and tall versions\n",
    "#     for height, legend_gap in [(600, 220), (800, 350)]:\n",
    "#         fig.update_layout(title_text=title, title_x=0.5, height=height, width=1600, coloraxis=dict(colorscale='Bluered_r'), barmode='stack', legend_tracegroupgap=legend_gap)\n",
    "#         if height == 800:\n",
    "#             fig.show()\n",
    "\n",
    "#         # Save to disk\n",
    "#         outpath = CURRENT_ANALYSIS_SAVE_DIR / f\"workload_stats_2x2/workload_stats_2x2_{deployment}_{run_date}_{height}px.png\"\n",
    "#         outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "#         fig.write_image(outpath)\n",
    "\n",
    "#         print(\"Saved to\", outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTU Stats vs expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptu_only_df = all_logs[(all_logs[\"deployment_type\"] == DEPLOYMENT_TYPE.PTU)].copy()\n",
    "ptu_only_df[\"max_ptu_rpm_achieved_for_workload\"] = ptu_only_df.groupby([\"deployment\", \"token_profile\"])[\"rpm\"].transform(\"max\")\n",
    "ptu_only_df[\"max_ptu_tpm_achieved_for_workload\"] = ptu_only_df.groupby([\"deployment\", \"token_profile\"])[\"tpm_total\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max TPM vs expected\n",
    "df = ptu_only_df[ptu_only_df[\"tpm_total\"] == ptu_only_df[\"max_ptu_tpm_achieved_for_workload\"]].copy()\n",
    "# Remove runs where avg concurrency was the same for the same token profile (remove duplicates)\n",
    "df = df.groupby(\"workload_name_observed\").first().reset_index().set_index(\"human_name\").sort_values(\n",
    "    [\"observed_prompt_vs_gen_token_ratio\",\"context_tokens\", \"max_tokens\"], ascending = [True, True, True]\n",
    ")[[\"context_tpr_avg\", \"max_tokens\", \"gen_tpr_avg\", \"observed_prompt_vs_gen_token_ratio\", \"exp_ptu_tpm\", \"tpm_total\", \"total_tpm_relative_to_expected\"]]\n",
    "# df.rename(columns={\"context_tpr_avg\": \"context_tokens_per_request\", \"max_tokens\": \"max_tokens_per_request\", \"gen_tpr_avg\": \"average_generated_tokens\", \"exp_ptu_tpm\": \"expected_ptu_tpm\"}, inplace=True)\n",
    "df.rename(columns=COL_RENAME_MAPPER, inplace=True)\n",
    "out = apply_pretty_formatting(df, cols_to_ignore=[\"Average Context Tokens/Request\", \"Max Tokens\", \"Average Generation Tokens/Request\"])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max Concurrency vs expected\n",
    "df = ptu_only_df[ptu_only_df[\"max_ptu_rpm_achieved_for_workload\"] == ptu_only_df[\"rpm\"]].copy()\n",
    "# Remove runs where avg concurrency was the same for the same token profile (remove duplicates)\n",
    "df = df.groupby(\"workload_name_observed\").first().reset_index().set_index(\"human_name\").sort_values(\n",
    "    [\"observed_prompt_vs_gen_token_ratio\",\"context_tokens\", \"max_tokens\"], ascending = [True, True, True]\n",
    ")[[\"context_tpr_avg\", \"max_tokens\", \"gen_tpr_avg\", \"observed_prompt_vs_gen_token_ratio\", \"exp_ptu_rpm\", \"max_ptu_rpm_achieved_for_workload\", \"ptu_rpm_relative_to_expected\"]]\n",
    "# df.rename(columns={\"context_tpr_avg\": \"context_tokens_per_request\", \"max_tokens\": \"max_tokens_per_request\", \"gen_tpr_avg\": \"average_generated_tokens\", \"exp_ptu_rpm\": \"expected_ptu_rpm\"}, inplace=True)\n",
    "df.rename(columns=COL_RENAME_MAPPER, inplace=True)\n",
    "out = apply_pretty_formatting(df, cols_to_ignore=[\"Average Context Tokens/Request\", \"Max Tokens\", \"Average Generation Tokens/Request\"])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTU TPM by ratio of prompt to generation tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "temp_df = ptu_only_df.copy()\n",
    "# Filter to runs with highest TPM\n",
    "temp_df = temp_df.sort_values([\"context_tpr_avg\", \"max_tokens\", \"tpm_total\"], ascending=(False, True, False)).groupby([\"token_profile\"]).first().reset_index()\n",
    "# Reorder from lowest ratio to highest\n",
    "temp_df = temp_df.sort_values([\"observed_prompt_vs_gen_token_ratio\"], ascending=False)\n",
    "\n",
    "# Format data\n",
    "# temp_df[\"total_tpm_relative_to_expected\"] = temp_df[\"total_tpm_relative_to_expected\"].apply(lambda x: round(x, 2))\n",
    "# temp_df[\"total_tpm_relative_to_expected\"] = temp_df[\"total_tpm_relative_to_expected\"] / 100\n",
    "total_tpm_plot_limit = max(temp_df[\"exp_ptu_tpm\"].max() * 1.1, temp_df[\"tpm_total\"].max() * 1.1)\n",
    "\n",
    "# temp_df[\"observed_prompt_vs_gen_token_ratio\"] = temp_df[\"observed_prompt_vs_gen_token_ratio\"].apply(lambda x: round(x * 100, 2))\n",
    "tpm_vs_expected_plot_limit = max(temp_df[\"total_tpm_relative_to_expected\"].max() * 1.1, 1.1)\n",
    "\n",
    "temp_df[\"throttled_rpm\"] = temp_df[\"throttled_rpm\"].apply(lambda x: round(x * 100))\n",
    "temp_df[\"rate\"] = temp_df[\"rate\"].apply(lambda x: \"None\" if pd.isna(x) else str(int(x)))\n",
    "\n",
    "custom_data_cols = [\"workload_name\", \"token_profile\", \"rate\", \"clients\", \"rpm\", \"throttled_rpm\", \"avg_concurrency\", \"e2e_avg\"]\n",
    "hover_template_str = \"<br>\".join([f\"{col.replace('_', ' ').title()}: %{{customdata[{i}]}}\" for i, col in enumerate(custom_data_cols)])\n",
    "# 'Workload Regime:%{customdata[0]} <br>Token Profile:%{customdata[1]} <br>Token Profile:%{customdata[2]} <br>Token Profile:%{customdata[3]} <br>RPM:%{customdata[4]} <br>Throttled RPM:%{customdata[5]} <br>  '\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "# Add secondary (hidden) trace with % of expected TPM on right\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=temp_df[\"observed_prompt_vs_gen_token_ratio\"].values.tolist(),\n",
    "        y=temp_df[\"total_tpm_relative_to_expected\"].values.tolist(),\n",
    "        mode=\"markers\",\n",
    "        # Hide from plot so we only get the y-axis labels\n",
    "        marker_opacity=0,\n",
    "        showlegend=False,\n",
    "        ),\n",
    "    secondary_y=True\n",
    ")\n",
    "# Add main trace with actual TPM values on left (last, so it is on top)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=temp_df[\"observed_prompt_vs_gen_token_ratio\"].values.tolist(),\n",
    "        y=temp_df[\"tpm_total\"].values.tolist(),\n",
    "        mode=\"markers+text\",\n",
    "        name=\"Max TPM by Workload\",\n",
    "        customdata=temp_df[custom_data_cols].values,\n",
    "        hovertemplate=hover_template_str,\n",
    "        text=temp_df[\"token_profile\"].values.tolist(),\n",
    "        textposition=\"bottom center\",\n",
    "        marker=dict(size=8, color=\"red\"),\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "# Add trendline\n",
    "time_size_regr = LinearRegression()\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly_features.fit_transform(temp_df[[\"observed_prompt_vs_gen_token_ratio\"]])\n",
    "X = X[:, [1,2]]\n",
    "\n",
    "time_size_regr.fit(X, temp_df[\"tpm_total\"].values.reshape(-1, 1))\n",
    "trendline_values = time_size_regr.predict(X).reshape(-1).tolist()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=temp_df[\"observed_prompt_vs_gen_token_ratio\"].values.tolist(),\n",
    "        y=trendline_values,\n",
    "        mode=\"lines\",\n",
    "        name=\"TPM Best Fit\",\n",
    "        line=dict(color='blue', width=2, dash='5px'),\n",
    "        # showlegend=False,\n",
    "        # marker=dict(color='rgba(255, 0, 0, 0)')\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "# Add dotted line for congigured PTU TPM (min and max). Use both a plot and a shape to get a legend and a full height vline\n",
    "# exp_max_ptu_tpm = int(temp_df[\"exp_max_tpm\"].iloc[0])\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=temp_df[\"exp_ptu_tpm\"],\n",
    "        x=temp_df[\"observed_prompt_vs_gen_token_ratio\"].values.tolist(),\n",
    "        name=\"Expected PTU TPM\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color='purple', width=2, dash='5px'),\n",
    "        marker=dict(size=8, color='rgba(255, 0, 0, 0)')\n",
    "        ),\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_xaxes(title_text=\"Observed Prompt vs Gen Token Ratio\")\n",
    "fig.update_yaxes(title_text=\"Max Total TPM Achieved\", range = [0, total_tpm_plot_limit], showgrid=False, secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"TPM Relative to Expected\", range = [0, tpm_vs_expected_plot_limit], secondary_y=True)\n",
    "\n",
    "title = f\"TPM by Prompt:Generation Token Ratio. Model: {temp_df['human_name_w_tpm'].iloc[0]}\"\n",
    "fig.update_layout(title_text=title, title_x=0.5, height=500, width=1000, coloraxis=dict(colorscale='Bluered_r'), xaxis_tickformat = '.0%', yaxis2_tickformat = '.0%')\n",
    "fig.show()\n",
    "\n",
    "# Save to disk\n",
    "outpath = CURRENT_ANALYSIS_SAVE_DIR / \"plots/ptu_max_tpm_vs_expected.png\"\n",
    "outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "fig.write_image(outpath)\n",
    "\n",
    "print(\"Saved to\", outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoai_benchmark_tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
